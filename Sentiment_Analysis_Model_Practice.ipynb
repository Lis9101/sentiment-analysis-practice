{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The practice is performed following this tutorial: https://www.makeuseof.com/create-sentiment-analysis-model/, which used Trip Advisor Hotel Reviews dataset from Kaggle to build the sentiment analysis model\n",
        "\n",
        "Dataset used to train the model in this practice: https://www.kaggle.com/datasets/abhi8923shriv/sentiment-analysis-dataset, which is is a Tweet Polarity dataset that is intented for sentiment analysis"
      ],
      "metadata": {
        "id": "A5bmAOGYlEto"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install tensorflow scikit-learn pandas numpy pickle5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "tx3Qhq3Zlqfy",
        "outputId": "74f4516b-69a9-4131-eb31-0e2e41819c01"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: pickle5 in /usr/local/lib/python3.10/dist-packages (0.0.11)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.62.2)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "c1KLi9yiksd9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "import pickle5 as pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "QjFdi3WolZrL",
        "outputId": "c05c573a-0039-483d-c762-b4666f42d28f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the Dataset"
      ],
      "metadata": {
        "id": "NWj74n3um26d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "df_test = pd.read_csv(\"/content/drive/MyDrive/Sentiment Analysis Dataset/test.csv\", encoding='latin1')\n",
        "df_train = pd.read_csv(\"/content/drive/MyDrive/Sentiment Analysis Dataset/train.csv\", encoding='latin1')\n",
        "\n",
        "# Select only 'text' and 'sentiment' columns\n",
        "df_test = df_test[['text', 'sentiment']]\n",
        "df_train = df_train[['text', 'sentiment']]"
      ],
      "metadata": {
        "id": "OR07SXiylc_f"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first 5 rows of the datasets\n",
        "print(df_test.head())\n",
        "print(df_train.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "0Hzy0A8RmqZB",
        "outputId": "861deac4-94a8-48c9-c27a-683a2adbe7d4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text sentiment\n",
            "0  Last session of the day  http://twitpic.com/67ezh   neutral\n",
            "1   Shanghai is also really exciting (precisely -...  positive\n",
            "2  Recession hit Veronique Branquinho, she has to...  negative\n",
            "3                                        happy bday!  positive\n",
            "4             http://twitpic.com/4w75p - I like it!!  positive\n",
            "                                                text sentiment\n",
            "0                I`d have responded, if I were going   neutral\n",
            "1      Sooo SAD I will miss you here in San Diego!!!  negative\n",
            "2                          my boss is bullying me...  negative\n",
            "3                     what interview! leave me alone  negative\n",
            "4   Sons of ****, why couldn`t they put them on t...  negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ],
      "metadata": {
        "id": "fH6v6Ywim7FD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values in the 'text' column\n",
        "missing_values_train = df_train['text'].isnull().sum()\n",
        "print(\"Number of missing values in 'text' column of training dataset:\", missing_values_train)\n",
        "missing_values_test = df_test['text'].isnull().sum()\n",
        "print(\"Number of missing values in 'text' column of testing dataset:\", missing_values_test)\n",
        "\n",
        "# Drop rows with missing values\n",
        "df_train = df_train.dropna(subset=['text'])\n",
        "df_test = df_train.dropna(subset=['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "3rl08LicsfHL",
        "outputId": "5df5e8dc-1731-48a8-93d2-e77773bcbc6c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of missing values in 'text' column of training dataset: 1\n",
            "Number of missing values in 'text' column of testing dataset: 1281\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization and Padding\n",
        "tokenizer = Tokenizer(num_words=5000, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(df_train['text'])\n",
        "word_index = tokenizer.word_index\n",
        "sequences_train = tokenizer.texts_to_sequences(df_train['text'])\n",
        "sequences_test = tokenizer.texts_to_sequences(df_test['text'])\n",
        "padded_sequences_train = pad_sequences(sequences_train, maxlen=100, truncating='post')\n",
        "padded_sequences_test = pad_sequences(sequences_test, maxlen=100, truncating='post')"
      ],
      "metadata": {
        "id": "R7MvAb9Pm8gc"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert sentiment labels to one-hot encoded vectors\n",
        "train_sentiment_labels = pd.get_dummies(df_train['sentiment']).values\n",
        "test_sentiment_labels = pd.get_dummies(df_test['sentiment']).values"
      ],
      "metadata": {
        "id": "DMAsHQ3Aot5K"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into features and labels\n",
        "x_train = padded_sequences_train\n",
        "y_train = train_sentiment_labels\n",
        "x_test = padded_sequences_test\n",
        "y_test = test_sentiment_labels"
      ],
      "metadata": {
        "id": "SrnHK9S-sI92"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating and Training the Neural Network"
      ],
      "metadata": {
        "id": "iWA8om9dozI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the Neural Network\n",
        "model = Sequential()\n",
        "model.add(Embedding(5000, 100, input_length=100))\n",
        "model.add(Conv1D(64, 5, activation='relu'))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Ca48FIOFo2Zg",
        "outputId": "0a5bcfe6-0a3e-4e1f-f2f2-0619892cf492"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 100, 100)          500000    \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 96, 64)            32064     \n",
            "                                                                 \n",
            " global_max_pooling1d (Glob  (None, 64)                0         \n",
            " alMaxPooling1D)                                                 \n",
            "                                                                 \n",
            " dense (Dense)               (None, 32)                2080      \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 32)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 3)                 99        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 534243 (2.04 MB)\n",
            "Trainable params: 534243 (2.04 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the Neural Network\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "jpwfS1xrpEF-",
        "outputId": "b917eda4-9ece-4dcb-db0d-7dd1dab01c7a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "859/859 [==============================] - 48s 55ms/step - loss: 0.8239 - accuracy: 0.6266 - val_loss: 0.5814 - val_accuracy: 0.7631\n",
            "Epoch 2/10\n",
            "859/859 [==============================] - 37s 43ms/step - loss: 0.6240 - accuracy: 0.7488 - val_loss: 0.4723 - val_accuracy: 0.8157\n",
            "Epoch 3/10\n",
            "859/859 [==============================] - 45s 52ms/step - loss: 0.5343 - accuracy: 0.7874 - val_loss: 0.3764 - val_accuracy: 0.8609\n",
            "Epoch 4/10\n",
            "859/859 [==============================] - 35s 41ms/step - loss: 0.4345 - accuracy: 0.8337 - val_loss: 0.2760 - val_accuracy: 0.9015\n",
            "Epoch 5/10\n",
            "859/859 [==============================] - 37s 43ms/step - loss: 0.3349 - accuracy: 0.8767 - val_loss: 0.2050 - val_accuracy: 0.9328\n",
            "Epoch 6/10\n",
            "859/859 [==============================] - 40s 46ms/step - loss: 0.2544 - accuracy: 0.9081 - val_loss: 0.1353 - val_accuracy: 0.9571\n",
            "Epoch 7/10\n",
            "859/859 [==============================] - 41s 48ms/step - loss: 0.1938 - accuracy: 0.9311 - val_loss: 0.1063 - val_accuracy: 0.9670\n",
            "Epoch 8/10\n",
            "859/859 [==============================] - 37s 43ms/step - loss: 0.1526 - accuracy: 0.9477 - val_loss: 0.0722 - val_accuracy: 0.9784\n",
            "Epoch 9/10\n",
            "859/859 [==============================] - 36s 41ms/step - loss: 0.1221 - accuracy: 0.9590 - val_loss: 0.0623 - val_accuracy: 0.9804\n",
            "Epoch 10/10\n",
            "859/859 [==============================] - 35s 41ms/step - loss: 0.1077 - accuracy: 0.9658 - val_loss: 0.0516 - val_accuracy: 0.9828\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a49098bfdf0>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the Performance of the Trained Model\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "y_pred = np.argmax(model.predict(x_test), axis=-1)\n",
        "y_true = np.argmax(y_test, axis=-1)\n",
        "# Calculate accuracy score\n",
        "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
        "# Calculate F1-score\n",
        "print(\"F1-score:\", f1_score(y_true, y_pred, average='macro'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "_6pNTzRSwgzK",
        "outputId": "c2d4eb03-e54d-498a-a560-def392597f56"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "859/859 [==============================] - 8s 9ms/step\n",
            "Accuracy: 0.9828238719068413\n",
            "F1-score: 0.9832633729384904\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the Model\n",
        "model.save('my_sentiment_analysis_model.h5')\n",
        "with open('tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "_igsLwpnwkNG",
        "outputId": "7150f0cb-9760-4b9b-d954-0802c128061c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using the Model to Classify the Sentiment of Given Text\n",
        "\n"
      ],
      "metadata": {
        "id": "VNiE0pRuwxhE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model and tokenizer\n",
        "import keras\n",
        "\n",
        "model = keras.models.load_model('my_sentiment_analysis_model.h5')\n",
        "with open('tokenizer.pickle', 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)"
      ],
      "metadata": {
        "id": "BNdwKJbIw4NZ"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to predict the sentiment of input text\n",
        "def predict_sentiment(text, model, tokenizer):\n",
        "    # Tokenize and pad the input text\n",
        "    text_sequence = tokenizer.texts_to_sequences([text])\n",
        "    text_sequence = pad_sequences(text_sequence, maxlen=100)\n",
        "\n",
        "    # Make a prediction using the trained model\n",
        "    predicted_rating = model.predict(text_sequence)[0]\n",
        "\n",
        "    # Map the predicted sentiment index to its corresponding label\n",
        "    sentiment_mapping = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
        "    predicted_index = np.argmax(predicted_rating)\n",
        "    predicted_sentiment_label = sentiment_mapping[predicted_index]\n",
        "\n",
        "    return predicted_index, predicted_sentiment_label"
      ],
      "metadata": {
        "id": "m21uTnK9w9D7"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive_texts = [\n",
        "    \"I loved the new book. It's amazing!\",\n",
        "    \"The weather today is beautiful.\",\n",
        "    \"I'm feeling great today.\",\n",
        "    \"I had a fantastic time at the party last night!\",\n",
        "    \"I'm really excited about the upcoming event.\",\n",
        "    \"The customer service was excellent!\",\n",
        "    \"I feel so happy right now.\",\n",
        "    \"The meeting went well.\",\n",
        "    \"The hotel room was clean and comfortable.\",\n",
        "    \"I'm proud of my achievements.\"\n",
        "]\n",
        "print(len(positive_texts))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "iZBs6Xk70P9b",
        "outputId": "123e8e90-3c02-46d5-baaa-fbd93d39b5e8"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for text in positive_texts:\n",
        "    predicted_index, predicted_sentiment = predict_sentiment(text, model, tokenizer)\n",
        "    print(\"Text:\", text)\n",
        "    print(\"Predicted Sentiment Index:\", predicted_index)\n",
        "    print(\"Predicted Sentiment Label:\", predicted_sentiment)\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "pmO8SqrJ1QOg",
        "outputId": "9e26c1b7-8601-4f7b-852d-af58948d12e0"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 33ms/step\n",
            "Text: I loved the new book. It's amazing!\n",
            "Predicted Sentiment Index: 2\n",
            "Predicted Sentiment Label: positive\n",
            "\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "Text: The weather today is beautiful.\n",
            "Predicted Sentiment Index: 2\n",
            "Predicted Sentiment Label: positive\n",
            "\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "Text: I'm feeling great today.\n",
            "Predicted Sentiment Index: 2\n",
            "Predicted Sentiment Label: positive\n",
            "\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "Text: I had a fantastic time at the party last night!\n",
            "Predicted Sentiment Index: 2\n",
            "Predicted Sentiment Label: positive\n",
            "\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Text: I'm really excited about the upcoming event.\n",
            "Predicted Sentiment Index: 2\n",
            "Predicted Sentiment Label: positive\n",
            "\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Text: The customer service was excellent!\n",
            "Predicted Sentiment Index: 2\n",
            "Predicted Sentiment Label: positive\n",
            "\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Text: I feel so happy right now.\n",
            "Predicted Sentiment Index: 2\n",
            "Predicted Sentiment Label: positive\n",
            "\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Text: The meeting went well.\n",
            "Predicted Sentiment Index: 1\n",
            "Predicted Sentiment Label: neutral\n",
            "\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Text: The hotel room was clean and comfortable.\n",
            "Predicted Sentiment Index: 1\n",
            "Predicted Sentiment Label: neutral\n",
            "\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Text: I'm proud of my achievements.\n",
            "Predicted Sentiment Index: 2\n",
            "Predicted Sentiment Label: positive\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "negative_texts = [\n",
        "    \"The service at the restaurant was terrible.\",\n",
        "    \"The traffic was awful this morning.\",\n",
        "    \"The food tasted awful.\",\n",
        "    \"I'm so disappointed with the product quality.\",\n",
        "    \"The flight got delayed again.\",\n",
        "    \"I can't stand this waiting.\",\n",
        "    \"I'm tired of all this drama.\",\n",
        "    \"I'm annoyed by all the noise outside.\",\n",
        "    \"The internet connection is so slow.\",\n",
        "    \"I'm so frustrated with this project.\"\n",
        "]\n",
        "print(len(negative_texts))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "7WhS_RfyG8bO",
        "outputId": "9c690766-bdd8-4c18-aaef-13ca83b346e2"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for text in negative_texts:\n",
        "    predicted_index, predicted_sentiment = predict_sentiment(text, model, tokenizer)\n",
        "    print(\"Text:\", text)\n",
        "    print(\"Predicted Sentiment Index:\", predicted_index)\n",
        "    print(\"Predicted Sentiment Label:\", predicted_sentiment)\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "90uuQDPF1VwD",
        "outputId": "03bb5d84-6039-4a6c-80b2-021d2d0516ff"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 56ms/step\n",
            "Text: The service at the restaurant was terrible.\n",
            "Predicted Sentiment Index: 0\n",
            "Predicted Sentiment Label: negative\n",
            "\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "Text: The traffic was awful this morning.\n",
            "Predicted Sentiment Index: 0\n",
            "Predicted Sentiment Label: negative\n",
            "\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "Text: The food tasted awful.\n",
            "Predicted Sentiment Index: 0\n",
            "Predicted Sentiment Label: negative\n",
            "\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "Text: I'm so disappointed with the product quality.\n",
            "Predicted Sentiment Index: 1\n",
            "Predicted Sentiment Label: neutral\n",
            "\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "Text: The flight got delayed again.\n",
            "Predicted Sentiment Index: 0\n",
            "Predicted Sentiment Label: negative\n",
            "\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "Text: I can't stand this waiting.\n",
            "Predicted Sentiment Index: 0\n",
            "Predicted Sentiment Label: negative\n",
            "\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Text: I'm tired of all this drama.\n",
            "Predicted Sentiment Index: 0\n",
            "Predicted Sentiment Label: negative\n",
            "\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "Text: I'm annoyed by all the noise outside.\n",
            "Predicted Sentiment Index: 0\n",
            "Predicted Sentiment Label: negative\n",
            "\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "Text: The internet connection is so slow.\n",
            "Predicted Sentiment Index: 0\n",
            "Predicted Sentiment Label: negative\n",
            "\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "Text: I'm so frustrated with this project.\n",
            "Predicted Sentiment Index: 0\n",
            "Predicted Sentiment Label: negative\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "neutral_texts = [\n",
        "    \"I am going to the store to buy some groceries.\",\n",
        "    \"The meeting is scheduled for 2 PM in the conference room.\",\n",
        "    \"I need to finish this report by the end of the day.\",\n",
        "    \"The new software update includes several bug fixes and improvements.\",\n",
        "    \"I'm planning to take a vacation next month.\",\n",
        "    \"The book I'm reading is quite interesting.\",\n",
        "    \"I'm going to try out a new recipe for dinner tonight.\",\n",
        "    \"I'm considering joining a yoga class to improve my flexibility.\",\n",
        "    \"The Industrial Revolution marked a significant turning point in human history.\",\n",
        "    \"The discovery of penicillin by Alexander Fleming revolutionized the field of medicine.\"\n",
        "]\n",
        "print(len(neutral_texts))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "oKgso0kfGZCa",
        "outputId": "ec204ab2-c683-4525-88a7-e21d30ee94ca"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for text in neutral_texts:\n",
        "    predicted_index, predicted_sentiment = predict_sentiment(text, model, tokenizer)\n",
        "    print(\"Text:\", text)\n",
        "    print(\"Predicted Sentiment Index:\", predicted_index)\n",
        "    print(\"Predicted Sentiment Label:\", predicted_sentiment)\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "MFFx4Og_1ZPM",
        "outputId": "10276186-59f2-4046-88a0-0c9fbbfb5950"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 27ms/step\n",
            "Text: I am going to the store to buy some groceries.\n",
            "Predicted Sentiment Index: 1\n",
            "Predicted Sentiment Label: neutral\n",
            "\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "Text: The meeting is scheduled for 2 PM in the conference room.\n",
            "Predicted Sentiment Index: 1\n",
            "Predicted Sentiment Label: neutral\n",
            "\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "Text: I need to finish this report by the end of the day.\n",
            "Predicted Sentiment Index: 1\n",
            "Predicted Sentiment Label: neutral\n",
            "\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "Text: The new software update includes several bug fixes and improvements.\n",
            "Predicted Sentiment Index: 1\n",
            "Predicted Sentiment Label: neutral\n",
            "\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "Text: I'm planning to take a vacation next month.\n",
            "Predicted Sentiment Index: 1\n",
            "Predicted Sentiment Label: neutral\n",
            "\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "Text: The book I'm reading is quite interesting.\n",
            "Predicted Sentiment Index: 2\n",
            "Predicted Sentiment Label: positive\n",
            "\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "Text: I'm going to try out a new recipe for dinner tonight.\n",
            "Predicted Sentiment Index: 1\n",
            "Predicted Sentiment Label: neutral\n",
            "\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Text: I'm considering joining a yoga class to improve my flexibility.\n",
            "Predicted Sentiment Index: 2\n",
            "Predicted Sentiment Label: positive\n",
            "\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "Text: The Industrial Revolution marked a significant turning point in human history.\n",
            "Predicted Sentiment Index: 1\n",
            "Predicted Sentiment Label: neutral\n",
            "\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "Text: The discovery of penicillin by Alexander Fleming revolutionized the field of medicine.\n",
            "Predicted Sentiment Index: 1\n",
            "Predicted Sentiment Label: neutral\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the results, the model is struggling with identifying neutral texts, especially differentially between positive and neutral texts.\n",
        "\n",
        "In the positive text set, 8/10 texts are labeled correctly, with the incorrect one labeled as neutral. In the negative text set, 9/10 texts are labeled correctly, with the incorrect one labeled as neutral. In the neutral text set, 8/10 texts are labeled correctly, and the remaining 2 texts are mislabeled as positive.\n",
        "\n",
        "When analyzing the dataset, I found that it contains more neutral tweets (40.5% of the tweets are labeled neutral). The trained model has decent accuracy score and F1-score, but it struggles to accurately identify neutral input text. This may be due to some problems with the quality of the dataset and the labeling process for neutral tweets."
      ],
      "metadata": {
        "id": "CRF3ZCP4AJdP"
      }
    }
  ]
}